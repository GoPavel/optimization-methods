{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import sys; sys.path.append('../src/')\n",
    "import one_dim_search.dichotomy as dichot\n",
    "import one_dim_search.fib as fib\n",
    "import one_dim_search.linear as lin\n",
    "import one_dim_search.golden as gold\n",
    "from descent.grad import gradient_descent_iter, get_constant_step_chooser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_func(*, cond: float, n: float):\n",
    "    min_v = 1\n",
    "    max_v = cond\n",
    "    assert max_v / min_v == cond\n",
    "    diag = [min_v, max_v]\n",
    "    while len(diag) < n:\n",
    "        diag.append(random.uniform(min_v, max_v))\n",
    "    \n",
    "#     m = np.diag(diag)\n",
    "    b = np.random.randn(n)\n",
    "\n",
    "    def f(arg):\n",
    "        return np.sum(diag * arg ** 2 - b.dot(arg))\n",
    "\n",
    "    def f_grad(arg):\n",
    "        return diag * arg * 2 - b\n",
    "    \n",
    "    return f, f_grad, diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze(step_chooser, eps=1e-3):\n",
    "    data = []\n",
    "    for cond in tqdm(np.linspace(1, 1000, 20)):\n",
    "        for n in [5, 10, 20, 50, 100]:\n",
    "            f, f_grad, _ = generate_func(cond=cond, n=n)\n",
    "            it = gradient_descent_iter(\n",
    "                f=f, f_grad=f_grad, eps=eps,\n",
    "                start=np.random.randn(n),\n",
    "                step_chooser=step_chooser,\n",
    "                _verbose=1000\n",
    "            )\n",
    "            data.append({\n",
    "                'cond': cond,\n",
    "                'n': n,\n",
    "                'cnt': sum(1 for _ in it)\n",
    "            })\n",
    "    fig = px.line(data, x='cond', y='cnt', color='n')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze(get_constant_step_chooser(1e-4), eps=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_step_chooser(one_dim_search):\n",
    "    def step_chooser(f, x_k, cur_grad):\n",
    "        phi = lambda h: f(x_k - h * cur_grad)\n",
    "        l, r = lin.search(0, delta=1e-4, f=phi, eps=1e-5, multiplier=2)\n",
    "        l, r = one_dim_search(l, r, f=phi, eps=1e-5)\n",
    "        return (l + r) / 2\n",
    "    return step_chooser\n",
    "\n",
    "analyze(generic_step_chooser(gold.search))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrix(condition_number, n):\n",
    "    r = math.sqrt(condition_number)\n",
    "    A = np.random.randn(n, n)\n",
    "    u, s, v = np.linalg.svd(A)\n",
    "    h, l = np.max(s), np.min(s)  # highest and lowest eigenvalues (h / l = current cond number)\n",
    "\n",
    "    # linear stretch: f(x) = a * x + b, f(h) = h, f(l) = h/r, cond number = h / (h/r) = r\n",
    "    def f(x):\n",
    "        return h * (1 - ((r - 1) / r) / (h - l) * (h - x))\n",
    "\n",
    "    new_s = f(s)\n",
    "    new_A = (u * new_s) @ v.T  # make inverse transformation (here cond number is sqrt(k))\n",
    "    new_A = new_A @ new_A.T  # make matrix symmetric and positive semi-definite (cond number is just k)\n",
    "    assert np.isclose(np.linalg.cond(new_A), condition_number)\n",
    "    return new_A\n",
    "\n",
    "\n",
    "def number_of_iters(cond, n_vars, step_chooser, n_checks=100):\n",
    "    avg_iters = 0\n",
    "    for _ in range(n_checks):\n",
    "        A = create_matrix(cond, n_vars)\n",
    "        b = np.random.randn(len(A))\n",
    "        init_x = np.random.randn(len(A))\n",
    "        f = lambda x: x.dot(A).dot(x) - b.dot(x)\n",
    "        f_grad = lambda x: (A + A.T).dot(x) - b\n",
    "\n",
    "        # print(f(np.linalg.inv(A+A.T).dot(b))) -- optimal value\n",
    "\n",
    "        trace = gradient_descent(f, f_grad, init_x, step_chooser, 'value')\n",
    "\n",
    "        avg_iters += len(trace)\n",
    "    return avg_iters / n_checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, m = generate_func(cond=3, n=4)\n",
    "m = np.diag(m)\n",
    "print(m)\n",
    "np.linalg.cond(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = create_matrix(3, 4)\n",
    "print(m)\n",
    "np.linalg.cond(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen2(cond, n):\n",
    "    A = create_matrix(cond, n)\n",
    "    b = np.random.randn(n)\n",
    "    def f(arg):\n",
    "        return arg.dot(A).dot(x) - b.dot(x)\n",
    "    return f\n",
    "\n",
    "    def f_grad(arg):\n",
    "        return diag * arg * 2 - b\n",
    "    return f\n",
    "\n",
    "    return f, f_grad, m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze2(step_chooser, eps=1e-3):\n",
    "    data = []\n",
    "    for cond in tqdm(np.linspace(1, 1000, 20)):\n",
    "        for n in [5, 10, 20, 50, 100]:\n",
    "            f, f_grad, _ = gen2(cond=cond, n=n)\n",
    "            it = gradient_descent_iter(\n",
    "                f=f, f_grad=f_grad, eps=eps,\n",
    "                start=np.random.randn(n),\n",
    "                step_chooser=step_chooser,\n",
    "                _verbose=1000\n",
    "            )\n",
    "            data.append({\n",
    "                'cond': cond,\n",
    "                'n': n,\n",
    "                'cnt': sum(1 for _ in it)\n",
    "            })\n",
    "    fig = px.line(data, x='cond', y='cnt', color='n')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generic_step_chooser(one_dim_search):\n",
    "    def step_chooser(f, x_k, cur_grad):\n",
    "        phi = lambda h: f(x_k - h * cur_grad)\n",
    "        l, r = lin.search(0, delta=1e-4, f=phi, eps=1e-5, multiplier=2)\n",
    "        l, r = one_dim_search(l, r, f=phi, eps=1e-5)\n",
    "        return (l + r) / 2\n",
    "    return step_chooser\n",
    "\n",
    "analyze(generic_step_chooser(gold.search))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
